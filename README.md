# Awesome LLM Self-Reflection [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

Inspired by the [awesome-embodied-vision](https://github.com/rxlqn/awesome-embodied-vision)
## <a name="contributing"></a> Contributing
When sending PRs, please put the new paper at the correct chronological position as the following format: <br>

```
* **Paper Title** <br>
*Author(s)* <br>
Conference, Year. [[Paper]](link) [[Code]](link) [[Website]](link)
```

## <a name="papers"></a> Papers
* **Reflexion: Language Agents with Verbal Reinforcement Learning** <br>
*Shinn, Noah and Cassano, Federico and Labash, Beck and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu* <br>
arxiv, 2023. [[Paper]](https://arxiv.org/abs/2303.11366)

* **SELF-REFINE: ITERATIVE REFINEMENT WITH SELF-FEEDBACK** <br>
*Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and et al.* <br>
arxiv, 2023. [[Paper]](https://arxiv.org/abs/2303.17651)

* **Large Language Models Can Self-Improve** <br>
*Huang, Jiaxin and Gu, ShixiangShane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei* <br>
arxiv, 2022. [[Paper]](https://arxiv.org/abs/2210.11610)

* **Teaching Large Language Models to Self-Debug** <br>
*Huang, Jiaxin and Gu, ShixiangShane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei* <br>
arxiv, 2023. [[Paper]](https://arxiv.org/abs/2304.05128)

* **SELFCHECK: USING LLMS TO ZERO-SHOT CHECK THEIR OWN STEP-BY-STEP REASONING** <br>
*Miao, Ning and Teh, YeeWhye and Rainforth, Tom* <br>
arxiv, 2023. [[Paper]](https://arxiv.org/abs/2308.00436)

* **ReAct: Synergizing Reasoning and Acting in Language Models** <br>
*Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan* <br>
arxiv, 2022. [[Paper]](https://arxiv.org/abs/2210.03629)
