# Awesome LLM Self-Reflection [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

Inspired by the [awesome-embodied-vision](https://github.com/rxlqn/awesome-embodied-vision)
## <a name="contributing"></a> Contributing
When sending PRs, please put the new paper at the correct chronological position as the following format: <br>

```
* **Paper Title** <br>
*Author(s)* <br>
Conference, Year. [[Paper]](link) [[Code]](link) [[Website]](link)
```

## <a name="papers"></a> Papers
* **Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies** <br>
*Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang Wang* <br>
arxiv, 2023. [[Paper]](https://arxiv.org/abs/2308.03188)

* **Reflexion: Language Agents with Verbal Reinforcement Learning** <br>
*Shinn, Noah and Cassano, Federico and Labash, Beck and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu* <br>
arxiv, 2023. [[Paper]](https://arxiv.org/abs/2303.11366)

* **SELF-REFINE: ITERATIVE REFINEMENT WITH SELF-FEEDBACK** <br>
*Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and et al.* <br>
arxiv, 2023. [[Paper]](https://arxiv.org/abs/2303.17651)

* **Large Language Models Can Self-Improve** <br>
*Huang, Jiaxin and Gu, ShixiangShane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei* <br>
arxiv, 2022. [[Paper]](https://arxiv.org/abs/2210.11610)

* **Teaching Large Language Models to Self-Debug** <br>
*Huang, Jiaxin and Gu, ShixiangShane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei* <br>
arxiv, 2023. [[Paper]](https://arxiv.org/abs/2304.05128)

* **SELFCHECK: USING LLMS TO ZERO-SHOT CHECK THEIR OWN STEP-BY-STEP REASONING** <br>
*Miao, Ning and Teh, YeeWhye and Rainforth, Tom* <br>
arxiv, 2023. [[Paper]](https://arxiv.org/abs/2308.00436)

* **ReAct: Synergizing Reasoning and Acting in Language Models** <br>
*Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan* <br>
arxiv, 2022. [[Paper]](https://arxiv.org/abs/2210.03629)

* **Self-Verification Improves Few-Shot Clinical Information Extraction** <br>
*Gero, Zelalem and Singh, Chandan and Cheng, Hao and Naumann, Tristan and Galley, Michel and Gao, Jianfeng and Poon, Hoifung* <br>
arxiv, 2023. [[Paper]](https://arxiv.org/pdf/2306.00024)

* **Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification** <br>
*Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li* <br>
arxiv, 2023. [[Paper]](https://arxiv.org/abs/2308.07921)

* **Shepherd: A Critic for Language Model Generation** <br>
*Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O'Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz* <br>
arxiv, 2023. [[Paper]](https://arxiv.org/abs/2308.04592)

* **Reinforced Self-Training (ReST) for Language Modeling** <br>
*Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, Nando de Freitas* <br>
arxiv, 2023. [[Paper]](https://arxiv.org/abs/2308.08998)


